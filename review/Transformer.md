# Attention is all you need
## 등장배경
![]()
* 기존 seq2seq 모델은 인코더-디코더 구조로 구성, 인코더는 입력 시퀀스를 고정된 크기의 context vecter로 압축하고, 디코더는 이 context vecter를 통해 출력 시퀀스를 만들었다.
* 하지만 인코더가 입력 시퀀스를 고정된 크기의 context vecter로 압축하는 과정에서 입력 시퀀스 정보가 일부 손실된다는 단점이 있었고, 이를 보완하는 것이 어텐션 메커니즘이다.
* 어텐션 메커니즘을 사용하더라도 여전히 RNN 계열 모델들은 다음 셀을 계산하기 위해 이전 셀의 값을 필요로 하기기 때문에 병렬처리가 안되서 속도가 느리다는 단점이 있다.
* 이러한 한계점을 극복하기위해 전적으로 어텐션 메커니즘에 의존해서 어텐션만으로 모델을 구성한 것이 트랜스포머이다.
* 트랜스포머는 RNN을 필요로 하지 않는다. 따라서 입력 문장이 주어지면 문장의 순서에 대한 정보가 부족하다.

## 포지셔널 인코딩(Positional Encoding)
* 트랜스포머는 RNN을 사용하지 않지만 기존의 seq2seq처럼 인코더에서 입력 시퀀스를 입력받고, 디코더에서 출력 시퀀스를 출력하는 인코더-디코더 구조를 유지한다. 
